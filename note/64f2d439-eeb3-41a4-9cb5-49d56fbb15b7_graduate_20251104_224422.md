# ðŸ“š 1744613183577

**Academic Level:** Graduate
**Document Type:** document
**Total Pages:** 0
**Total Chapters:** 1

---

## ðŸ“‹ Document Overview

These comprehensive study notes cover all content from the source document, organized hierarchically and formatted for optimal learning at the graduate level.



---

## ðŸ” Chapter 1: Pages 0-0

# Prompt Engineering
## Introduction

Prompt engineering is the process of designing high-quality prompts that guide Large Language Models (LLMs) to produce accurate outputs. It is an iterative process that involves tinkering to find the best prompt, optimizing prompt length, and evaluating a prompt's writing style and structure in relation to the task.

### Key Aspects of Prompt Engineering

*   **Model Selection**: Choosing the right model for the task, such as Gemini language models in Vertex AI, GPT, Claude, or open-source models like Gemma or LLaMA.
*   **Prompt Design**: Crafting a well-structured prompt that guides the LLM to produce the desired output.
*   **Model Configuration**: Adjusting settings like temperature, top-K, and top-P to control the LLM's output.

## LLM Output Configuration

LLMs have various configuration options that control the output. Effective prompt engineering requires setting these configurations optimally for the task.

### Output Length

*   **Definition**: The number of tokens to generate in a response.
*   **Importance**: Generating more tokens requires more computation, leading to higher energy consumption, potentially slower response times, and higher costs.
*   **Considerations**: Reducing the output length doesn't cause the LLM to become more stylistically or textually succinct; it just causes the LLM to stop predicting more tokens once the limit is reached.

### Sampling Controls

LLMs predict probabilities for what the next token could be, with each token in the LLM's vocabulary getting a probability. Those token probabilities are then sampled to determine what the next produced token will be.

#### Temperature

*   **Definition**: Controls the degree of randomness in token selection.
*   **Range**: Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results.
*   **Example**: A temperature of 0 (greedy decoding) is deterministic, where the highest probability token is always selected.

#### Top-K and Top-P

*   **Definition**:
    *   **Top-K**: Selects the top K most likely tokens from the model's predicted distribution.
    *   **Top-P** (nucleus sampling): Selects the top tokens whose cumulative probability does not exceed a certain value (P).
*   **Importance**: These sampling settings control the randomness and diversity of generated text.

### Putting it all Together

Choosing between top-K, top-P, temperature, and the number of tokens to generate depends on the specific application and desired outcome. The settings all impact one another.

*   **Interaction**: If temperature, top-K, and top-P are all available, tokens that meet both the top-K and top-P criteria are candidates for the next predicted token, and then temperature is applied to sample from the tokens that passed the top-K and top-P criteria.

## Prompting Techniques

LLMs are tuned to follow instructions and are trained on large amounts of data so they can understand a prompt and generate an answer. Specific techniques that take advantage of how LLMs are trained and how LLMs work will help get the relevant results from LLMs.

### General Prompting / Zero-Shot

*   **Definition**: A zero-shot prompt is the simplest type of prompt, providing a description of a task and some text for the LLM to get started with.
*   **Example**: A zero-shot prompt to classify movie reviews.

| Name | 1_1_movie_classification |  |  |
| :-- | :-- | :-- | :-- |
| Goal | Classify movie reviews as positive, neutral or negative. |  |  |

### One-Shot & Few-Shot

*   **Definition**: Providing one or a few examples of the task to help the LLM understand what is expected.
*   **Importance**: Helps the LLM generate more accurate outputs by providing context and examples.

### System, Contextual, and Role Prompting

*   **Definition**: Techniques used to provide context and guide the LLM's output.
*   **Types**:
    *   **System Prompting**: Provides a high-level description of the task or role.
    *   **Role Prompting**: Specifies the role or persona the LLM should adopt.
    *   **Contextual Prompting**: Provides context or background information to help the LLM understand the task.

## Advanced Prompting Techniques

### Step-Back Prompting

*   **Definition**: A technique used to help the LLM generate more accurate outputs by asking it to take a step back and consider the task from a different perspective.
*   **Importance**: Helps the LLM avoid getting stuck in a repetitive loop and generate more diverse outputs.

### Chain of Thought (CoT)

*   **Definition**: A technique used to help the LLM generate more accurate outputs by asking it to provide a step-by-step explanation of its reasoning.
*   **Importance**: Helps the LLM generate more transparent and explainable outputs.

### Self-Consistency

*   **Definition**: A technique used to help the LLM generate more accurate outputs by asking it to provide multiple responses to the same prompt and then selecting the most consistent one.
*   **Importance**: Helps the LLM generate more reliable and consistent outputs.

### Tree of Thoughts (ToT)

*   **Definition**: A technique used to help the LLM generate more accurate outputs by asking it to generate multiple possible responses to a prompt and then selecting the best one.
*   **Importance**: Helps the LLM generate more diverse and accurate outputs.

### ReAct (Reason & Act)

*   **Definition**: A technique used to help the LLM generate more accurate outputs by asking it to reason about the task and then take actions based on that reasoning.
*   **Importance**: Helps the LLM generate more accurate and actionable outputs.

## Best Practices

*   **Provide Examples**: Providing examples of the task or role can help the LLM understand what is expected.
*   **Design with Simplicity**: Keeping the prompt simple and concise can help the LLM generate more accurate outputs.
*   **Be Specific about the Output**: Specifying the desired output can help the LLM generate more accurate outputs.
*   **Use Instructions over Constraints**: Using instructions rather than constraints can help the LLM generate more accurate outputs.
*   **Control the Max Token Length**: Controlling the maximum token length can help prevent the LLM from generating too much output.
*   **Use Variables in Prompts**: Using variables in prompts can help make the prompt more flexible and reusable.
*   **Experiment with Input Formats and Writing Styles**: Experimenting with different input formats and writing styles can help find the most effective way to communicate with the LLM.

## Challenges and Limitations

*   **Repetition Loop Bug**: A common issue in LLMs where the model gets stuck in a cycle, repeatedly generating the same (filler) word, phrase, or sentence structure.
*   **Lack of Common Sense**: LLMs may not always have common sense or real-world experience, which can lead to inaccurate or nonsensical outputs.

## Conclusion

Prompt engineering is a crucial aspect of working with LLMs, as it can significantly impact the accuracy and quality of the outputs. By understanding the different prompting techniques and best practices, users can create effective prompts that guide the LLM to produce accurate and relevant outputs. However, there are also challenges and limitations to consider, such as the repetition loop bug and the lack of common sense. By being aware of these challenges and limitations, users can take steps to mitigate them and create more effective prompts.

```mermaid
graph TD
    A[Prompting Techniques] --> B[General Prompting / Zero Shot]
    A --> C[One-Shot & Few-Shot]
    A --> D[System, Contextual, and Role Prompting]
    D --> E[System Prompting]
    D --> F[Role Prompting]
    D --> G[Contextual Prompting]
    G --> H[Step-back Prompting]
    G --> I[Chain of Thought (CoT)]
    G --> J[Self-Consistency]
    G --> K[Tree of Thoughts (ToT)]
    G --> L[ReAct (Reason & Act)]
    A --> M[Automatic Prompt Engineering]
    A --> N[Code Prompting]
    N --> O[Prompts for Writing Code]
    N --> P[Prompts for Explaining Code]
    N --> Q[Prompts for Translating Code]
    N --> R[Prompts for Debugging and Reviewing Code]
    A --> S[Multimodal Prompting]
    A --> T[Best Practices]
    T --> U[Provide Examples]
    T --> V[Design with Simplicity]
    T --> W[Be Specific about Output]
    T --> X[Use Instructions over Constraints]
    T --> Y[Control Max Token Length]
    T --> Z[Use Variables in Prompts]
```