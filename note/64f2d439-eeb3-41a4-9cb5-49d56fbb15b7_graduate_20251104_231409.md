# ðŸ“š 1744613183577

**Academic Level:** Graduate
**Document Type:** document
**Total Pages:** 0
**Total Chapters:** 1

---

## ðŸ“‹ Document Overview

These comprehensive study notes cover all content from the source document, organized hierarchically and formatted for optimal learning at the graduate level.



---

## ðŸ” Chapter 1: Pages 0-0

# Prompt Engineering
## Introduction
Prompt engineering is the process of designing high-quality prompts that guide Large Language Models (LLMs) to produce accurate outputs. This process involves optimizing prompt length, evaluating a prompt's writing style and structure in relation to the task, and tinkering with various configurations of a LLM.

## LLM Output Configuration
LLMs come with various configuration options that control the LLM's output. Effective prompt engineering requires setting these configurations optimally for a specific task.

### Output Length
The number of tokens to generate in a response is an important configuration setting. Generating more tokens requires more computation from the LLM, leading to higher energy consumption, potentially slower response times, and higher costs.

* Reducing the output length of the LLM doesn't cause the LLM to become more stylistically or textually succinct in the output it creates.
* It just causes the LLM to stop predicting more tokens once the limit is reached.

### Sampling Controls
LLMs predict probabilities for what the next token could be, with each token in the LLM's vocabulary getting a probability. Those token probabilities are then sampled to determine what the next produced token will be.

#### Temperature
Temperature controls the degree of randomness in token selection.

* Lower temperatures are good for prompts that expect a more deterministic response.
* Higher temperatures can lead to more diverse or unexpected results.
* A temperature of 0 (greedy decoding) is deterministic: the highest probability token is always selected.

#### Top-K and Top-P
Top-K and top-P (also known as nucleus sampling) are two sampling settings used in LLMs to restrict the predicted next token to come from tokens with the top predicted probabilities.

* Top-K sampling selects the top K most likely tokens from the model's predicted distribution.
* Top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value (P).

## Putting it all Together
Choosing between top-K, top-P, temperature, and the number of tokens to generate depends on the specific application and desired outcome.

* The settings all impact one another.
* It's also important to make sure you understand how your chosen model combines the different sampling settings together.

### Guidelines for Starting Points
* A temperature of .2, top-P of .95, and top-K of 30 will give relatively coherent results that can be creative but not excessively so.
* A temperature of .9, top-P of .99, and top-K of 40 for especially creative results.
* A temperature of .1, top-P of .9, and top-K of 20 for less creative results.
* A temperature of 0 for tasks that always have a single correct answer.

## Prompting Techniques
LLMs are tuned to follow instructions and are trained on large amounts of data so they can understand a prompt and generate an answer.

### General Prompting / Zero Shot
A zero-shot prompt is the simplest type of prompt. It only provides a description of a task and some text for the LLM to get started with.

* This input could be anything: a question, a start of a story, or instructions.
* The name zero-shot stands for 'no examples'.

### One-Shot & Few-Shot
One-shot and few-shot prompting involve providing one or more examples of the task to help the LLM understand what is expected.

* This can be especially helpful for tasks that require a specific format or style.

### System, Contextual, and Role Prompting
System, contextual, and role prompting involve providing additional context or information to help the LLM understand the task and generate a more accurate response.

#### System Prompting
System prompting involves providing a system message or prompt that sets the tone and direction for the conversation.

* This can be especially helpful for tasks that require a specific tone or style.

#### Role Prompting
Role prompting involves providing a role or persona for the LLM to adopt.

* This can be especially helpful for tasks that require a specific perspective or point of view.

#### Contextual Prompting
Contextual prompting involves providing additional context or information to help the LLM understand the task and generate a more accurate response.

* This can be especially helpful for tasks that require a specific understanding of the context.

## Advanced Prompting Techniques
There are several advanced prompting techniques that can be used to improve the accuracy and effectiveness of LLM responses.

### Step-Back Prompting
Step-back prompting involves asking the LLM to take a step back and re-evaluate the task or prompt.

* This can be especially helpful for tasks that require a more nuanced or detailed understanding.

### Chain of Thought (CoT)
Chain of thought (CoT) prompting involves asking the LLM to provide a step-by-step explanation or reasoning for its response.

* This can be especially helpful for tasks that require a more transparent or explainable response.

### Self-Consistency
Self-consistency prompting involves asking the LLM to generate multiple responses to the same prompt and then evaluating the consistency of the responses.

* This can be especially helpful for tasks that require a more accurate or reliable response.

### Tree of Thoughts (ToT)
Tree of thoughts (ToT) prompting involves asking the LLM to generate a tree-like structure of possible responses or solutions.

* This can be especially helpful for tasks that require a more creative or exploratory response.

### ReAct (Reason & Act)
ReAct (reason & act) prompting involves asking the LLM to provide a response that is based on both reasoning and action.

* This can be especially helpful for tasks that require a more practical or applicable response.

## Code Prompting
Code prompting involves using prompts to generate code or to explain existing code.

### Prompts for Writing Code
Prompts for writing code involve providing a specific task or requirement and asking the LLM to generate code that meets that requirement.

* This can be especially helpful for tasks that require a specific programming language or style.

### Prompts for Explaining Code
Prompts for explaining code involve providing a piece of code and asking the LLM to explain how it works.

* This can be especially helpful for tasks that require a more transparent or understandable response.

### Prompts for Translating Code
Prompts for translating code involve providing a piece of code in one language and asking the LLM to translate it into another language.

* This can be especially helpful for tasks that require a specific programming language or style.

### Prompts for Debugging and Reviewing Code
Prompts for debugging and reviewing code involve providing a piece of code and asking the LLM to identify errors or suggest improvements.

* This can be especially helpful for tasks that require a more accurate or reliable response.

## Best Practices
There are several best practices that can be used to improve the effectiveness of prompt engineering.

### Provide Examples
Providing examples can be especially helpful for tasks that require a specific format or style.

### Design with Simplicity
Designing with simplicity involves using clear and concise language to help the LLM understand the task.

### Be Specific about the Output
Being specific about the output involves providing clear and detailed information about what is expected.

### Use Instructions over Constraints
Using instructions over constraints involves providing clear and direct instructions rather than relying on constraints or limitations.

### Control the Max Token Length
Controlling the max token length involves setting a specific limit on the number of tokens that can be generated.

### Use Variables in Prompts
Using variables in prompts involves using placeholders or variables to provide more flexibility and adaptability.

### Experiment with Input Formats and Writing Styles
Experimenting with input formats and writing styles involves trying different approaches to see what works best.

### For Few-Shot Prompting with Classification Tasks, Mix Up the Classes
For few-shot prompting with classification tasks, mixing up the classes involves providing a diverse set of examples to help the LLM understand the task.

### Adapt to Model Updates
Adapting to model updates involves being aware of changes to the LLM and adjusting the prompts accordingly.

### Experiment with Output Formats
Experimenting with output formats involves trying different approaches to see what works best.

## JSON Repair
JSON repair involves using prompts to repair or fix JSON data.

## Working with Schemas
Working with schemas involves using prompts to generate or validate data against a specific schema.

## Conclusion
Prompt engineering is a critical component of working with LLMs. By understanding the different prompting techniques and best practices, users can improve the accuracy and effectiveness of LLM responses.

```mermaid
graph TD
    A[Prompting Techniques] --> B[General Prompting / Zero-Shot]
    A --> C[One-Shot & Few-Shot]
    A --> D[System, Contextual, and Role Prompting]
    D --> E[System Prompting]
    D --> F[Role Prompting]
    D --> G[Contextual Prompting]
    G --> H[Step-Back Prompting]
    G --> I[Chain of Thought (CoT)]
    I --> J[Self-Consistency]
    I --> K[Tree of Thoughts (ToT)]
    I --> L[ReAct (Reason & Act)]
    A --> M[Automatic Prompt Engineering]
    A --> N[Code Prompting]
    N --> O[Prompts for Writing Code]
    N --> P[Prompts for Explaining Code]
    N --> Q[Prompts for Translating Code]
    N --> R[Prompts for Debugging and Reviewing Code]
    A --> S[Multimodal Prompting]
    A --> T[Best Practices]
```