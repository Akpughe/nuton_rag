# ðŸ“š 1744613183577

**Academic Level:** Graduate
**Document Type:** document
**Total Pages:** 0
**Total Chapters:** 1

---

## ðŸ“‹ Document Overview

These comprehensive study notes cover all content from the source document, organized hierarchically and formatted for optimal learning at the graduate level.



---

## ðŸ” Chapter 1: Pages 0-0

# Prompt Engineering
## Introduction

Prompt engineering is the process of designing high-quality prompts that guide Large Language Models (LLMs) to produce accurate outputs. This process involves optimizing prompt length, evaluating a prompt's writing style and structure in relation to the task, and tinkering to find the best prompt.

### Key Aspects of Prompt Engineering

* **Model selection**: Choose a suitable LLM for the task, such as Gemini, GPT, Claude, or open-source models like Gemma or LLaMA.
* **Prompt design**: Craft a clear and effective prompt that guides the LLM to produce the desired output.
* **Model configuration**: Adjust settings like temperature, top-K, and top-P to control the LLM's output.

## LLM Output Configuration

LLMs have various configuration options that control the output. Effective prompt engineering requires setting these configurations optimally for the task.

### Output Length

* **Token generation**: The number of tokens to generate in a response.
* **Computation and costs**: Generating more tokens requires more computation, leading to higher energy consumption, slower response times, and higher costs.

### Sampling Controls

LLMs predict probabilities for the next token, and sampling controls determine how these probabilities are processed to choose a single output token.

#### Temperature

* **Definition**: Controls the degree of randomness in token selection.
* **Values**: Lower temperatures (e.g., 0) are deterministic, while higher temperatures (e.g., 1) lead to more diverse or unexpected results.

#### Top-K and Top-P

* **Top-K sampling**: Selects the top K most likely tokens from the model's predicted distribution.
* **Top-P sampling**: Selects the top tokens whose cumulative probability does not exceed a certain value (P).

### Putting it all Together

* **Choosing settings**: Experiment with different combinations of temperature, top-K, top-P, and output length to achieve the desired output.
* **Interactions between settings**: Understand how the chosen model combines different sampling settings.

## Prompting Techniques

LLMs are tuned to follow instructions and are trained on large amounts of data. Specific techniques can help get relevant results from LLMs.

### General Prompting / Zero-Shot

* **Definition**: A zero-shot prompt provides a description of a task and some text for the LLM to get started with, without any examples.
* **Example**: A zero-shot prompt to classify movie reviews.

| Name | 1_1_movie_classification |  |  |
| --- | --- | --- | --- |
| Goal | Classify movie reviews as positive, neutral, or negative. |  |  |

### One-Shot and Few-Shot Prompting

* **Definition**: One-shot and few-shot prompts provide one or multiple examples of the task, respectively.
* **Example**: A one-shot prompt to classify movie reviews.

| Name | 1_2_movie_classification_one_shot |  |  |
| --- | --- | --- | --- |
| Goal | Classify movie reviews as positive, neutral, or negative. |  |  |
| Example | "I loved this movie!" -> Positive |  |  |

### System, Contextual, and Role Prompting

* **System prompting**: Provides context about the system or task.
* **Contextual prompting**: Provides context about the specific task or input.
* **Role prompting**: Specifies the role or persona the LLM should adopt.

## Step-Back Prompting

* **Definition**: A technique to help the LLM take a step back and re-evaluate the task or input.

## Chain of Thought (CoT)

* **Definition**: A technique to encourage the LLM to provide a chain of thoughts or reasoning.

## Self-Consistency

* **Definition**: A technique to ensure the LLM's output is consistent across multiple responses.

## Tree of Thoughts (ToT)

* **Definition**: A technique to generate a tree of possible thoughts or responses.

## ReAct (Reason & Act)

* **Definition**: A technique to combine reasoning and acting in a single prompt.

## Automatic Prompt Engineering

* **Definition**: The use of automated methods to optimize prompts.

## Code Prompting

* **Definition**: The use of prompts to generate, explain, translate, or debug code.

### Prompts for Writing Code

* **Example**: A prompt to generate code for a simple calculator.

### Prompts for Explaining Code

* **Example**: A prompt to explain a piece of code.

### Prompts for Translating Code

* **Example**: A prompt to translate code from one language to another.

### Prompts for Debugging and Reviewing Code

* **Example**: A prompt to debug and review code.

## Best Practices

* **Provide examples**: Use examples to illustrate the task or input.
* **Design with simplicity**: Keep prompts simple and concise.
* **Be specific about the output**: Clearly specify the desired output.
* **Use instructions over constraints**: Use instructions to guide the LLM rather than constraints.
* **Control the max token length**: Set a reasonable maximum token length.
* **Use variables in prompts**: Use variables to make prompts more flexible.
* **Experiment with input formats and writing styles**: Try different input formats and writing styles.
* **Adapt to model updates**: Update prompts as the model evolves.

## JSON Repair

* **Definition**: A technique to repair JSON output.

## Working with Schemas

* **Definition**: The use of schemas to structure and validate output.

## Conclusion

Prompt engineering is an iterative process that requires careful consideration of the model, prompt design, and model configuration. By following best practices and using various prompting techniques, you can create effective prompts that guide LLMs to produce accurate outputs.

```mermaid
graph TD
    A[Prompting Techniques] --> B[General Prompting / Zero-Shot]
    A --> C[One-Shot & Few-Shot]
    A --> D[System, Contextual, and Role Prompting]
    D --> E[System Prompting]
    D --> F[Role Prompting]
    D --> G[Contextual Prompting]
    G --> H[Step-Back Prompting]
    G --> I[Chain of Thought (CoT)]
    I --> J[Self-Consistency]
    I --> K[Tree of Thoughts (ToT)]
    I --> L[ReAct (Reason & Act)]
    A --> M[Automatic Prompt Engineering]
    A --> N[Code Prompting]
    N --> O[Prompts for Writing Code]
    N --> P[Prompts for Explaining Code]
    N --> Q[Prompts for Translating Code]
    N --> R[Prompts for Debugging and Reviewing Code]
    A --> S[Multimodal Prompting]
    A --> T[Best Practices]
    T --> U[Provide Examples]
    T --> V[Design with Simplicity]
    T --> W[Be Specific about Output]
    T --> X[Use Instructions over Constraints]
    T --> Y[Control Max Token Length]
    T --> Z[Use Variables in Prompts]
```