# ðŸ“š 1744613183577

**Academic Level:** Graduate
**Document Type:** document
**Total Pages:** 0
**Total Chapters:** 1

---

## ðŸ“‹ Document Overview

These comprehensive study notes cover all content from the source document, organized hierarchically and formatted for optimal learning at the graduate level.



---

## ðŸ” Chapter 1: Pages 0-0

# Prompt Engineering
## Introduction
Prompt engineering is the process of designing high-quality prompts that guide Large Language Models (LLMs) to produce accurate outputs. This process involves optimizing prompt length, evaluating a prompt's writing style and structure in relation to the task, and tinkering with various configurations of a LLM.

### Importance of Prompt Engineering
Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model's ability to provide meaningful output. Crafting the most effective prompt can be complicated, and many aspects of a prompt affect its efficacy.

## LLM Output Configuration
Once a model is chosen, the next step is to figure out the model configuration. Most LLMs come with various configuration options that control the LLM's output.

### Output Length
An important configuration setting is the number of tokens to generate in a response. Generating more tokens requires more computation from the LLM, leading to higher energy consumption, potentially slower response times, and higher costs.

* Reducing the output length of the LLM doesn't cause the LLM to become more stylistically or textually succinct in the output it creates.
* It just causes the LLM to stop predicting more tokens once the limit is reached.

### Sampling Controls
LLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the next token could be, with each token in the LLM's vocabulary getting a probability.

* **Temperature**: controls the degree of randomness in token selection.
	+ Lower temperatures are good for prompts that expect a more deterministic response.
	+ Higher temperatures can lead to more diverse or unexpected results.
* **Top-K and top-P**: two sampling settings used in LLMs to restrict the predicted next token to come from tokens with the top predicted probabilities.

## Prompting Techniques
LLMs are tuned to follow instructions and are trained on large amounts of data so they can understand a prompt and generate an answer.

### General Prompting / Zero Shot
A zero-shot prompt is the simplest type of prompt. It only provides a description of a task and some text for the LLM to get started with.

* **Example**: classify movie reviews as positive, neutral, or negative.

| Name | 1_1_movie_classification |  |  |
| :-- | :-- | :-- | :-- |
| Goal | Classify movie reviews as positive, neutral or negative. |  

### One-Shot & Few-Shot
One-shot and few-shot prompting involves providing one or more examples of the task to the LLM.

* **One-shot**: provide one example of the task.
* **Few-shot**: provide multiple examples of the task.

### System, Contextual, and Role Prompting
System, contextual, and role prompting involves providing context and setting a role for the LLM.

* **System prompting**: provide a system message that sets the tone and context for the conversation.
* **Role prompting**: provide a role for the LLM to play.
* **Contextual prompting**: provide context for the LLM to understand the task.

## Advanced Prompting Techniques
### Step-Back Prompting
Step-back prompting involves asking the LLM to take a step back and provide a more general response.

### Chain of Thought (CoT)
Chain of thought prompting involves asking the LLM to provide a step-by-step explanation of its thought process.

### Self-Consistency
Self-consistency prompting involves asking the LLM to provide multiple responses to the same prompt and then selecting the most consistent response.

### Tree of Thoughts (ToT)
Tree of thoughts prompting involves asking the LLM to generate multiple possible responses and then selecting the best one.

### ReAct (Reason & Act)
ReAct prompting involves asking the LLM to reason and act in a specific context.

### Automatic Prompt Engineering
Automatic prompt engineering involves using algorithms to automatically generate and optimize prompts.

## Code Prompting
Code prompting involves using prompts to generate code.

* **Prompts for writing code**: use specific and clear language to describe the task.
* **Prompts for explaining code**: use clear and concise language to explain the code.

## Best Practices
### Provide Examples
Providing examples can help the LLM understand the task and generate more accurate responses.

### Design with Simplicity
Designing prompts with simplicity in mind can help reduce errors and improve performance.

### Be Specific about the Output
Being specific about the output can help the LLM generate more accurate and relevant responses.

### Use Instructions over Constraints
Using instructions over constraints can help the LLM understand the task and generate more accurate responses.

### Control the Max Token Length
Controlling the max token length can help prevent the LLM from generating too much text.

### Use Variables in Prompts
Using variables in prompts can help make the prompts more flexible and reusable.

### Experiment with Input Formats and Writing Styles
Experimenting with input formats and writing styles can help improve the performance of the LLM.

### Adapt to Model Updates
Adapting to model updates can help ensure that the prompts continue to work effectively.

## Conclusion
Prompt engineering is a critical component of working with LLMs. By understanding the different prompting techniques and best practices, users can generate more accurate and relevant responses from LLMs.

## Endnotes
* [1] Google's prompting guides
* [2] Google's Vertex AI Studio

## Definitions
> **Prompt engineering**: the process of designing high-quality prompts that guide LLMs to produce accurate outputs.
> **LLM**: Large Language Model.
> **Temperature**: controls the degree of randomness in token selection.
> **Top-K and top-P**: two sampling settings used in LLMs to restrict the predicted next token to come from tokens with the top predicted probabilities.

## Formulas
### Temperature
The temperature control can be understood in a similar way to the softmax function used in machine learning.

* `softmax(x) = exp(x) / Î£ exp(x)`

### Top-K and Top-P
Top-K sampling selects the top K most likely tokens from the model's predicted distribution.

* `P(x) = âˆ‘[k=1 to K] p(x_k)`

Top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value (P).

* `P(x) = âˆ‘[k=1 to n] p(x_k) where n is the largest k such that âˆ‘[k=1 to k] p(x_k) â‰¤ P`

## Mermaid Diagrams
```mermaid
graph LR;
    A[Prompt Engineering] --> B[LLM Output Configuration];
    B --> C[Prompting Techniques];
    C --> D[General Prompting / Zero Shot];
    C --> E[One-Shot & Few-Shot];
    C --> F[System, Contextual, and Role Prompting];
    C --> G[Advanced Prompting Techniques];
    G --> H[Step-Back Prompting];
    G --> I[Chain of Thought (CoT)];
    G --> J[Self-Consistency];
    G --> K[Tree of Thoughts (ToT)];
    G --> L[ReAct (Reason & Act)];
    G --> M[Automatic Prompt Engineering];
```

```mermaid
graph LR
    A[Prompt Engineering] --> B[LLM Output Configuration]
    A --> C[Prompting Techniques]
    C --> D[General Prompting / Zero Shot]
    C --> E[One-Shot & Few-Shot]
    C --> F[System, Contextual, and Role Prompting]
    F --> G[System Prompting]
    F --> H[Role Prompting]
    F --> I[Contextual Prompting]
    C --> J[Step-Back Prompting]
    C --> K[Chain of Thought (CoT)]
    C --> L[Self-Consistency]
    C --> M[Tree of Thoughts (ToT)]
    C --> N[ReAct (Reason & Act)]
    C --> O[Automatic Prompt Engineering]
    C --> P[Code Prompting]
    C --> Q[Multimodal Prompting]

    subgraph Best Practices
        BP[Best Practices] --> Provide Examples[Provide Examples]
        BP --> Design with Simplicity[Design with Simplicity]
        BP --> Be Specific[Be Specific about Output]
        BP --> Use Instructions[Use Instructions over Constraints]
        BP --> Control Max Token[Control Max Token Length]
        BP --> Use Variables[Use Variables in Prompts]
    end

    A --> BP
```